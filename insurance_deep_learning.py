# -*- coding: utf-8 -*-
"""Insurance-Deep-Learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-Y1b4LCurBo6OGm2Qwfi1_O1KOdBtTme

**About Dataset**

This dataset can be helpful in a simple yet illuminating study in understanding the risk underwriting in Health Insurance, the interplay of various attributes of the insured and see how they affect the insurance premium.

This dataset contains 1338 rows of insured data, where the Insurance charges are given against the following attributes of the insured: Age, Sex, BMI, Number of Children, Smoker and Region. There are no missing or undefined values in the dataset.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf

insurance = pd.read_csv("insurance.csv")
insurance.head()

print(insurance.columns)

insurance.isna().sum()

# Data is not missing

insurance.info()

insurance.sex, insurance.smoker, insurance.region

insurance.head()

# Now we need to convert these Columns with datatype object to numbers
insurance_encoded = pd.get_dummies(insurance)
insurance_encoded.head()

"""### Now we have converted our data into numbers"""

# Split the data into X and Y

X = insurance_encoded.drop("charges", axis=1)
Y = insurance_encoded["charges"]

X

Y

"""### Split our data into training and test sets"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)
len(X_train), len(X_test), len(X)

len(X) == len(X_train) + len(X_test)

X_train.shape

"""### Deep Learning Model"""

# Build a neural network (sort of like model_2 above)
tf.random.set_seed(42)

# 1. Create a model
insurance_model = tf.keras.Sequential([
    tf.keras.layers.Dense(10),
    tf.keras.layers.Dense(1)
])

# 2. Compile the model
insurance_model.compile(loss=tf.keras.losses.mae,
                        optimizer=tf.keras.optimizers.SGD(),
                        metrics=["mae"])

# 3. Fit the model
insurance_model.fit(X_train, y_train, epochs=100)

# Check the results of the insurance model on the test data
insurance_model.evaluate(X_test, y_test)

"""# To improve a model's performance we can either

Right now it looks like our model isn't performing too well... let's try and improve it!
1. Increase the number of layers
2. Increase the number of neurons per layer
3. Change the Optimizer function
4. Increase the number of epochs
5. Change the activation function
6. Increase the learning rate of the optimizer

*To try to improve our model, we'll run 2 experiments*:
1. Add an extra layer with more hidden units
2. Train for longer
3. Insert your own experiment here
"""

# Set random seed
tf.random.set_seed(42)

# 1. Create a model
insurance_model_2 = tf.keras.Sequential([
    tf.keras.layers.Dense(100),
    tf.keras.layers.Dense(10),
    tf.keras.layers.Dense(1)
])

# 2. Compile the model
insurance_model_2.compile(loss=tf.keras.losses.mae,
                          optimizer=tf.keras.optimizers.Adam(),
                          metrics=["mae"])

# 3. Fit the model
insurance_model_2.fit(X_train, y_train, epochs=100)

insurance_model_2.evaluate(X_test, y_test)

"""## Insuarance_model 3"""

# Set random seed
tf.random.set_seed(42)

# 1. Create the model (same as above)
insurance_model_3 = tf.keras.Sequential([
    tf.keras.layers.Dense(100),
    tf.keras.layers.Dense(10),
    tf.keras.layers.Dense(1)
])

insurance_model_3.compile(loss=tf.keras.losses.mae,
                          optimizer=tf.keras.optimizers.Adam(),
                          metrics=["mae"])

# 3. Fit the model
history = insurance_model_3.fit(X_train, y_train, epochs=200)

eval_model_3 = insurance_model_3.evaluate(X_test, y_test)
eval_model_2 = insurance_model_2.evaluate(X_test, y_test)
eval_model_1 = insurance_model.evaluate(X_test, y_test)

# Plot history (also known as a loss curve or a training curve)
pd.DataFrame(history.history).plot()
plt.ylabel("loss")
plt.xlabel("epochs")

"""**NOTE** : After seeing this curve it is clearly visible that loss decreases drastically when the range of epochs is between 0-110 but after that the loss does not decrease much.
So number of epochs should be max 110 to avoid overfitting.

# **Preprocessing data (normalization and standardization)**

## Normalizing the data set so that data in all columns is in the same range between 0-1
Normalization : Normalization is a technique often applied as part of data preparation for machine learning. The goal of normalization is to change the values of numeric columns in the dataset to use a common scale, without distorting differences in the
ranges of values or losing information.

In terms of scaling values, neural networks tend to prefer normalization.

If you're not sure on which to use, you could try both and see which performs better.
"""

X

X["age"].plot(kind="hist")
# This goes from 20 to 60

X["bmi"].plot(kind="hist")
# This goes from 15-55

# Read the insurance dataframe

insurance = pd.read_csv("insurance.csv")
insurance.head()

"""To prepare our new data we can borrow a few classes from scikit learn"""

from sklearn.compose import make_column_transformer
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder

# cREATE A COLUMN TRANSFORMER
ct = make_column_transformer(
    (MinMaxScaler(), ["age", "bmi", "children"]), # turn all values in these column between 0 and 1
    (OneHotEncoder(handle_unknown="ignore"), ["sex", "smoker", "region"])
)

# Create X and Y
new_X = insurance.drop("charges", axis=1)
new_Y = insurance["charges"]

# Build our train and test sets
X_train, X_test, y_train, y_test = train_test_split(new_X, new_Y)

# Fit the column transformer to our training data
ct.fit(X_train)

# Transform training and test data with normalization (MinMaxScaler) and OneHotEncoder
X_train_normal = ct.transform(X_train)
X_test_normal = ct.transform(X_test)

# What does our data look like now
X_train

X_train_normal

"""Beautiful! Our data has been normalized and one hot encoded. Now let's build a Neural network model"""

tf.random.set_seed(42)

# 1. Create the model
insurance_model_4 = tf.keras.Sequential([
    tf.keras.layers.Dense(100),
    tf.keras.layers.Dense(10),
    tf.keras.layers.Dense(1)
])

# 2. Compile the model
insurance_model_4.compile(loss=tf.keras.losses.mae,
                          optimizer=tf.keras.optimizers.Adam(),
                          metrics=["mae"])

# 3. Fit the model
insurance_model_4.fit(X_train_normal, y_train, epochs=100)

# Evaluate our insurance model trained on normalized data
eval_model_4 = insurance_model_4.evaluate(X_test_normal, y_test)

eval_model_1, eval_model_2, eval_model_3, eval_model_4

insurance_model_4.summary()

